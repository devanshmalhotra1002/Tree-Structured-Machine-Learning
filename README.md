# Tree Based Models

## Topics Covered [refer to word file]

### Decision Tree Basics
1. **Elements:** Includes fundamental building blocks of decision trees, including nodes, branches, and leaves.
2. **Working:** Includes how decision trees make predictions by recursively splitting data based on specific features.
3. **Application:** Examine how decision trees are used for both classification (predicting categories) and regression (predicting continuous values)
4. **Evaluation:** Analyze the advantages and disadvantages of decision trees, including their interpretability and potential for overfitting.

### Pruning
1. We discuss the concept of pruning to prevent overfitting by controlling the complexity of the tree. This section covers:
2. Why Pruning is Necessary
3. Types of Pruning
   * Pre-Pruning
   * Post-Pruning
4. Cost Complexity Pruning [commonly used post-pruning]

### Ensemble Learning
1. **Core Concepts:** Introduce to the concept of ensemble learning, where multiple models are combined to improve overall performance.
2. **Bagging:** Explore bagging, a method that trains multiple decision trees on random subsets of data to improve prediction accuracy.
3. **Random Forest:** Delve deeper into random forests, a powerful ensemble method that utilizes bagging and feature randomness for robust predictions.
4. **Boosting:** Includes a comprehensive overview of boosting, another ensemble method that iteratively trains models by focusing on previously misclassified instances. This section covers:
   * AdaBoost
   * Gradient Boosting
   * Extreme Gradient Boosting
   * Advantages-Disadvantages & Applications
